# LLM provider configuration
# Provider: gemini (default), openai, anthropic, or ollama
LLM_PROVIDER=gemini
# Optional: override the default model for your provider
# LLM_MODEL=gemini-2.0-flash
# Optional: Ollama server URL (default: http://localhost:11434/v1)
# OLLAMA_BASE_URL=http://localhost:11434/v1

# LLM API keys (only the key for your chosen provider is required):
# Ollama requires no API key, just a running server and LLM_MODEL set.
# Gemini: https://aistudio.google.com/apikey
GEMINI_API_KEY=your-gemini-api-key-here
# OpenAI: https://platform.openai.com/api-keys
# OPENAI_API_KEY=your-openai-api-key-here
# Anthropic: https://console.anthropic.com/settings/keys
# ANTHROPIC_API_KEY=your-anthropic-api-key-here

# Search provider (required):
# Tavily: https://tavily.com
TAVILY_API_KEY=tvly-your-tavily-key-here
